# Feature Specification: Unmock AI Integration

**Feature Branch**: `[006-unmock-ai-integration]`
**Created**: 2025-11-05
**Status**: Implemented
**Implementation Date**: 2025-11-05
**Input**: User request: "Unmock AI functionality while keeping ads platforms and external APIs mocked"

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Real AI recommendation generation (Priority: P1)

As a marketing lead, I can receive AI-powered recommendations generated by a real LLM (Anthropic Claude or OpenAI) instead of hardcoded mock data, so that I get intelligent, context-aware insights based on actual campaign performance patterns.

**Why this priority**: Real AI analysis is the core value proposition of OmniAds. Without it, the system cannot provide genuinely intelligent recommendations.

**Independent Test**: Sync campaign data and verify that AI generates relevant recommendations that differ based on actual campaign metrics, not hardcoded scenarios.

**Acceptance Scenarios**:

1. **Given** campaign data has been synced, **When** AI analysis runs, **Then** the system calls a real LLM API (Anthropic Claude or OpenAI) with structured campaign data.
2. **Given** campaigns have different performance metrics, **When** recommendations are generated, **Then** the AI provides contextually relevant suggestions (not hardcoded) based on actual ROAS, spend, and conversion patterns.
3. **Given** the LLM API returns a response, **When** processing recommendations, **Then** the system correctly parses AI-generated insights into structured RecommendationType records.

---

### User Story 2 - AI-powered daily summaries via WhatsApp (Priority: P1)

As a marketing manager, I can receive AI-generated daily summaries that intelligently summarize the most critical insights and actions, so that I stay informed without manually reviewing dashboards.

**Why this priority**: WhatsApp summaries are a key touchpoint for users. AI-generated summaries provide more value than template-based messages.

**Independent Test**: Trigger daily summary generation and verify that the WhatsApp message contains AI-analyzed insights specific to the day's performance data.

**Acceptance Scenarios**:

1. **Given** recommendations have been generated, **When** the daily summary is created, **Then** the AI synthesizes insights into a concise, actionable WhatsApp message format.
2. **Given** there are high-priority recommendations, **When** the summary is generated, **Then** the AI highlights the top 3 most impactful actions with clear rationale.
3. **Given** campaign performance varies, **When** summaries are sent on different days, **Then** each summary reflects the actual day's data, not generic templates.

---

### User Story 3 - AI-powered global insights for dashboard (Priority: P2)

As a data analyst, I can see AI-generated global insights on the dashboard that provide strategic recommendations across all platforms, so that I understand cross-platform optimization opportunities.

**Why this priority**: Dashboard insights guide strategic decisions but can be delivered after core recommendation workflows are functional.

**Independent Test**: Load the dashboard and verify that the global insight is dynamically generated by AI based on current multi-platform performance data.

**Acceptance Scenarios**:

1. **Given** campaigns are running across Meta, Google, and TikTok, **When** the dashboard loads, **Then** the AI generates a strategic insight comparing platform performance and suggesting reallocation opportunities.
2. **Given** platform performance changes, **When** the dashboard is refreshed, **Then** the global insight adapts to reflect new conditions.
3. **Given** insufficient data (e.g., < 7 days), **When** insight generation is requested, **Then** the AI provides a fallback message explaining data limitations.

---

### User Story 4 - Configurable AI provider and fallback (Priority: P3)

As a system administrator, I can configure which AI provider to use (Anthropic Claude or OpenAI) and set up fallback behavior for API failures, so that the system remains resilient and cost-effective.

**Why this priority**: Provider flexibility and resilience are valuable but can be added after core AI integration is proven.

**Independent Test**: Switch AI provider via environment configuration and verify that recommendations are still generated correctly.

**Acceptance Scenarios**:

1. **Given** environment is configured for Anthropic Claude, **When** recommendations are generated, **Then** the system uses Claude API endpoints.
2. **Given** environment is configured for OpenAI, **When** recommendations are generated, **Then** the system uses OpenAI API endpoints.
3. **Given** the AI provider API fails, **When** recommendation generation is attempted, **Then** the system logs the error, retries with exponential backoff, and falls back to cached recommendations if available.

---

### Edge Cases

- What happens when AI API quota is exceeded? Implement exponential backoff, log errors, and fall back to the most recent successful recommendations with a timestamp warning.
- How does the system handle malformed or incomplete AI responses? Validate response structure with Zod schemas, retry once on validation failure, and log errors for monitoring.
- How to prevent runaway costs from excessive API calls? Implement request rate limiting per workspace (max 10 AI calls per hour) and caching of recommendations for 1 hour.
- What if campaign data is insufficient for meaningful AI analysis? Detect data quality issues (< 7 days history, < 100 impressions) and provide explicit fallback messaging to users.

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST integrate with real LLM APIs (Anthropic Claude API or OpenAI API) for recommendation generation, replacing all hardcoded mock logic in `ai-core.client.ts`.
- **FR-002**: System MUST send structured campaign, product, and sales data to the LLM with a well-designed prompt that elicits actionable recommendations in the six defined categories (PAUSE_CAMPAIGN, SCALE_CAMPAIGN, BUDGET_SHIFT, COMPETITOR_PRICE, PROMOTE_ORGANIC, CREATE_BUNDLE).
- **FR-003**: System MUST validate and parse LLM responses into structured `AIRecommendation` objects using schema validation (Zod or similar).
- **FR-004**: System MUST maintain backward compatibility with existing mock ads platform APIs (Google, Meta, TikTok, Twilio, Tienda Nube) which remain mocked for MVP.
- **FR-005**: System MUST implement error handling for LLM API failures including retry logic (exponential backoff), timeout handling (30s max), and fallback to cached recommendations.
- **FR-006**: System MUST support configurable AI provider selection via environment variables (`AI_PROVIDER=anthropic|openai`).
- **FR-007**: System MUST implement request rate limiting (max 10 AI calls per hour per workspace) and recommendation caching (1 hour TTL) to control costs.
- **FR-008**: System MUST log all AI API requests and responses (sanitized) for debugging, cost tracking, and quality monitoring.

### Non-Functional Requirements

- **NFR-001**: AI recommendation generation MUST complete within 10 seconds (95th percentile) to maintain responsive UX.
- **NFR-002**: System MUST handle at least 100 concurrent workspaces with AI-enabled recommendations without degradation.
- **NFR-003**: AI integration MUST be deployed without breaking existing functionality (zero downtime deployment).
- **NFR-004**: Sensitive data (API keys, campaign names, product details) MUST be handled securely and never logged in plaintext.

### Key Entities *(include if feature involves data)*

- **AICoreClient** (modified): Replaces mock implementation with real LLM API integration, adds provider abstraction layer.
- **AIRecommendation** (existing): No schema changes required, but validation logic is added.
- **AIRequestLog** (new): Tracks AI API usage per workspace (timestamp, provider, tokensUsed, latency, success, errorMessage).
- **RecommendationCache** (new): Stores recently generated recommendations (workspaceId, cacheKey, recommendations, expiresAt).

### Technical Architecture

**AI Provider Abstraction**:
```
├── AICoreClient (orchestrator)
├── AnthropicProvider (implements LLM provider interface)
├── OpenAIProvider (implements LLM provider interface)
└── PromptBuilder (constructs structured prompts)
```

**Prompt Design**:
- Structured JSON input with campaign metrics, product performance, sales data
- Clear instruction for six recommendation types with priority scoring
- Few-shot examples to guide output format
- Token optimization to stay within model limits (< 4K tokens input)

**Response Parsing**:
- Zod schema validation for AIRecommendation array
- Retry logic for malformed responses (1 retry attempt)
- Fallback to partial results if some recommendations fail validation

**Caching Strategy**:
- Cache key: `workspace:{id}:recommendations:{YYYYMMDD}`
- TTL: 1 hour (3600 seconds)
- Invalidation: On manual data sync trigger
- Storage: Redis (if available) or in-memory Map (fallback)

**Rate Limiting**:
- Sliding window: 10 requests per hour per workspace
- Implementation: Redis counter or in-memory sliding window
- Response: HTTP 429 with Retry-After header when limit exceeded

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: 100% of AI recommendation requests successfully call real LLM APIs (Anthropic or OpenAI) instead of returning hardcoded data.
- **SC-002**: 95% of AI-generated recommendations are successfully parsed and stored in the database without validation errors.
- **SC-003**: AI recommendation latency (API call + parsing) stays below 10 seconds for 95th percentile of requests.
- **SC-004**: Zero incidents of ads platform mock APIs being replaced (Google, Meta, TikTok, Twilio, Tienda Nube remain fully mocked).
- **SC-005**: AI cost per workspace stays below $5/month (tracked via AIRequestLog token usage and provider pricing).
- **SC-006**: System handles AI API failures gracefully with fallback to cached recommendations in 100% of failure scenarios.
- **SC-007**: Manual testing validates that AI recommendations are contextually relevant and differ based on input campaign data (not hardcoded).

## Implementation Notes

### Environment Variables Required

```env
# AI Provider Configuration
AI_PROVIDER=anthropic  # or 'openai'
ANTHROPIC_API_KEY=sk-ant-xxx
OPENAI_API_KEY=sk-xxx

# AI Behavior Configuration
AI_MAX_REQUESTS_PER_HOUR=10
AI_CACHE_TTL_SECONDS=3600
AI_REQUEST_TIMEOUT_MS=30000

# Keep existing mock API configurations unchanged
TWILIO_ACCOUNT_SID=mock_account_sid
TWILIO_AUTH_TOKEN=mock_auth_token
# ... (all other mock configs remain)
```

### Dependencies

- **New packages**:
  - `@anthropic-ai/sdk` - Anthropic Claude SDK
  - `openai` - OpenAI SDK
  - `zod` - Schema validation
  - `@nestjs/throttler` - Rate limiting (already installed, enable for AI endpoints)

### Files to Modify

1. **`backend/src/services/ai-core.client.ts`** - Complete rewrite to use real LLM APIs
2. **`backend/src/services/data-sync.service.ts`** - No changes needed (already calls AICoreClient)
3. **`backend/.env.example`** - Add AI provider configuration
4. **`backend/package.json`** - Add new dependencies

### Files to Create

1. **`backend/src/services/ai/anthropic.provider.ts`** - Anthropic Claude integration
2. **`backend/src/services/ai/openai.provider.ts`** - OpenAI integration
3. **`backend/src/services/ai/prompt-builder.ts`** - Prompt construction logic
4. **`backend/src/services/ai/response-parser.ts`** - LLM response validation and parsing
5. **`backend/src/database/entities/ai-request-log.entity.ts`** - AI usage tracking
6. **`backend/src/database/entities/recommendation-cache.entity.ts`** - Recommendation caching

### Testing Strategy

1. **Unit Tests**: Mock LLM provider responses and verify parsing logic
2. **Integration Tests**: Use test API keys to verify end-to-end AI flow
3. **Manual Testing**: Compare AI-generated recommendations against known campaign scenarios
4. **Smoke Test**: Verify ads platform mocks are still functioning (no real API calls)
